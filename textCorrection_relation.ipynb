{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95bafab-5428-429b-8397-3fd7eea9add4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Correction - fastText 활용한 OCR 오인식 보안 \n",
    "  \n",
    "[목적]\n",
    "- 주민등록증, 가족관계증명서 등 행정서류에서 관계(본인/자녀 등)를 추출하여 OCR 변환 시, 오인식으로 잘못된 관계정보 처리될 수 있음(e.g. 몬인, 지녀 등)\n",
    "- 관계와 관련된 단어를 fastText 통해 학습하여 **사전에 등록된 단어로 변환**하여 정확도 향상\n",
    "\n",
    "[활용 기술/라이브러리]\n",
    "- fastText : 페이스북에서 word2vec(구글)의 단점을 보완하면서 만들어낸 알고리즘\n",
    "- 문장 속 단어들의 조합으로 워드 임베딩을 하며, 이에 따라 학습에 사용된 적이 없는 단어에 대해서도 단어 벡터를 만들 수 있음\n",
    "- 형태학적 측면의 유사성 판단 가능  \n",
    "  &rarr; 우리는 **의미가 아닌 형태의 유사성으로만 판단하기 때문에 fastText 활용** \n",
    "\n",
    "[적용 내용]\n",
    "- 모델 학습 위한 말뭉치의 경우, 단어+자모 형태로 구성하여 형태 유사성 판단 강화(자모 추가 여부에 따라 미세하게 차이남)\n",
    "- 유사도 평가 시 단어-단어 비교 보다는 **자모분리한 단어와 자모분리한 단어를 비교**하면 원하는 결과 얻을 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f23cd-eeea-4b8b-b509-c11397a38852",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2738c5-6dd6-4be7-a59d-311bdbde2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import gensim.models.word2vec\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "import re\n",
    "import pandas as pd\n",
    "from soynlp.hangle import decompose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e15a12-d0fa-48ec-8698-12999f505821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec213347-b493-4a45-9d6a-09c8a83c5471",
   "metadata": {},
   "source": [
    "## fastText 모델\n",
    "### 학습데이터  \n",
    "[sample] 글자수에 따라 label 구분 임의 정의  \n",
    "__label__1 부  \n",
    "__label__2 본인  \n",
    "__label__3 배우자  \n",
    "__label__4 처고모부  \n",
    "__label__6 ㄱ  \n",
    "### 모델 생성 및 학습\n",
    "- model = FastText(학습데이터셋, min_count, ....)\n",
    "> min_count : 학습 시 지정한 숫자 보다 숫자보다 적게 발생하는 단어들은 학습하지 않음(빈도가 적은 단어들은 학습하지 않기 위함이나 우리는 문장 말뭉치가 아니여서 1로 지정)  \n",
    "  word_ngrams : default word_ngrams=1 => n-gram 사용, 0 => 미사용(word2vec과 동일)  \n",
    "  vector_size : 학습할 임베딩의 크기. 즉, 임베딩된 벡터의 차원  # 300이 적절  \n",
    "  epchs : default epchs=5 &rarr; epoch 늘려도 성능 큰 차이 없음. 오히려 예상한 결과과 다른 단어가 우선수위화 됨  \n",
    "  sg : default sg=0 => CBOW, if sg=1 => skip-gram (2가지 학습 알고리즘의 성능 차이는 크지 않음)  \n",
    "  workers : 학습시 사용하는 프로세스 개수  \n",
    "  window : context window 크기 # 5~10이 적절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683e2d0b-e7a2-4448-af96-916453290cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 16:35:12,198 : INFO : collecting all words and their counts\n",
      "2022-09-01 16:35:12,199 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-09-01 16:35:12,200 : INFO : collected 146 word types from a corpus of 280 raw words and 1 sentences\n",
      "2022-09-01 16:35:12,200 : INFO : Creating a fresh vocabulary\n",
      "2022-09-01 16:35:12,201 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 146 unique words (100.00% of original 146, drops 0)', 'datetime': '2022-09-01T16:35:12.201754', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-01 16:35:12,202 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 280 word corpus (100.00% of original 280, drops 0)', 'datetime': '2022-09-01T16:35:12.202192', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-01 16:35:12,203 : INFO : deleting the raw counts dictionary of 146 items\n",
      "2022-09-01 16:35:12,204 : INFO : sample=0.001 downsamples 146 most-common words\n",
      "2022-09-01 16:35:12,204 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 128.53145469697102 word corpus (45.9%% of prior 280)', 'datetime': '2022-09-01T16:35:12.204798', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-09-01 16:35:12,207 : INFO : estimated required memory for 146 words, 2000000 buckets and 300 dimensions: 2400440944 bytes\n",
      "2022-09-01 16:35:12,208 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus 생성\n",
      "학습\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 16:35:14,957 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-09-01T16:35:14.957673', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-09-01 16:35:14,958 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 146 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-09-01T16:35:14.958313', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-09-01 16:35:14,961 : INFO : EPOCH 0: training on 280 raw words (128 effective words) took 0.0s, 70276 effective words/s\n",
      "2022-09-01 16:35:14,965 : INFO : EPOCH 1: training on 280 raw words (130 effective words) took 0.0s, 164022 effective words/s\n",
      "2022-09-01 16:35:14,970 : INFO : EPOCH 2: training on 280 raw words (131 effective words) took 0.0s, 34560 effective words/s\n",
      "2022-09-01 16:35:14,973 : INFO : EPOCH 3: training on 280 raw words (119 effective words) took 0.0s, 102139 effective words/s\n",
      "2022-09-01 16:35:14,977 : INFO : EPOCH 4: training on 280 raw words (127 effective words) took 0.0s, 75387 effective words/s\n",
      "2022-09-01 16:35:14,980 : INFO : EPOCH 5: training on 280 raw words (125 effective words) took 0.0s, 104755 effective words/s\n",
      "2022-09-01 16:35:14,984 : INFO : EPOCH 6: training on 280 raw words (134 effective words) took 0.0s, 82688 effective words/s\n",
      "2022-09-01 16:35:14,988 : INFO : EPOCH 7: training on 280 raw words (128 effective words) took 0.0s, 60289 effective words/s\n",
      "2022-09-01 16:35:14,992 : INFO : EPOCH 8: training on 280 raw words (122 effective words) took 0.0s, 70208 effective words/s\n",
      "2022-09-01 16:35:14,996 : INFO : EPOCH 9: training on 280 raw words (134 effective words) took 0.0s, 97394 effective words/s\n",
      "2022-09-01 16:35:15,000 : INFO : EPOCH 10: training on 280 raw words (136 effective words) took 0.0s, 84566 effective words/s\n",
      "2022-09-01 16:35:15,004 : INFO : EPOCH 11: training on 280 raw words (128 effective words) took 0.0s, 63512 effective words/s\n",
      "2022-09-01 16:35:15,010 : INFO : EPOCH 12: training on 280 raw words (124 effective words) took 0.0s, 59047 effective words/s\n",
      "2022-09-01 16:35:15,014 : INFO : EPOCH 13: training on 280 raw words (125 effective words) took 0.0s, 67186 effective words/s\n",
      "2022-09-01 16:35:15,018 : INFO : EPOCH 14: training on 280 raw words (138 effective words) took 0.0s, 87097 effective words/s\n",
      "2022-09-01 16:35:15,022 : INFO : EPOCH 15: training on 280 raw words (132 effective words) took 0.0s, 87600 effective words/s\n",
      "2022-09-01 16:35:15,026 : INFO : EPOCH 16: training on 280 raw words (128 effective words) took 0.0s, 56948 effective words/s\n",
      "2022-09-01 16:35:15,032 : INFO : EPOCH 17: training on 280 raw words (128 effective words) took 0.0s, 66420 effective words/s\n",
      "2022-09-01 16:35:15,037 : INFO : EPOCH 18: training on 280 raw words (126 effective words) took 0.0s, 88112 effective words/s\n",
      "2022-09-01 16:35:15,041 : INFO : EPOCH 19: training on 280 raw words (127 effective words) took 0.0s, 57034 effective words/s\n",
      "2022-09-01 16:35:15,046 : INFO : EPOCH 20: training on 280 raw words (125 effective words) took 0.0s, 58545 effective words/s\n",
      "2022-09-01 16:35:15,051 : INFO : EPOCH 21: training on 280 raw words (127 effective words) took 0.0s, 60690 effective words/s\n",
      "2022-09-01 16:35:15,055 : INFO : EPOCH 22: training on 280 raw words (119 effective words) took 0.0s, 72973 effective words/s\n",
      "2022-09-01 16:35:15,058 : INFO : EPOCH 23: training on 280 raw words (123 effective words) took 0.0s, 92443 effective words/s\n",
      "2022-09-01 16:35:15,065 : INFO : EPOCH 24: training on 280 raw words (130 effective words) took 0.0s, 53112 effective words/s\n",
      "2022-09-01 16:35:15,066 : INFO : FastText lifecycle event {'msg': 'training on 7000 raw words (3194 effective words) took 0.1s, 29749 effective words/s', 'datetime': '2022-09-01T16:35:15.066123', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-09-01 16:35:15,070 : INFO : FastText lifecycle event {'params': 'FastText<vocab=146, vector_size=300, alpha=0.025>', 'datetime': '2022-09-01T16:35:15.070503', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-09-01 16:35:15,071 : INFO : FastText lifecycle event {'fname_or_handle': 'text_correciton_relation_model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-09-01T16:35:15.071686', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-09-01 16:35:15,072 : INFO : storing np array 'vectors_ngrams' to text_correciton_relation_model.wv.vectors_ngrams.npy\n",
      "2022-09-01 16:35:18,397 : INFO : not storing attribute buckets_word\n",
      "2022-09-01 16:35:18,397 : INFO : not storing attribute vectors\n",
      "2022-09-01 16:35:18,398 : INFO : not storing attribute cum_table\n",
      "2022-09-01 16:35:18,399 : INFO : saved text_correciton_relation_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 소요 시간 : 0.04544824899998545\n",
      "(146, 300)\n"
     ]
    }
   ],
   "source": [
    "corpus_path = 'relation_word.txt'\n",
    "model_name = 'text_correciton_relation_model'\n",
    "\n",
    "print('corpus 생성')\n",
    "corpus = gensim.models.word2vec.Text8Corpus(corpus_path) \n",
    "\n",
    "print(\"학습\")\n",
    "model = FastText(corpus, min_count=1, word_ngrams=1, vector_size = 300, epochs=25)\n",
    "model.save(model_name)\n",
    "\n",
    "print(f\"학습 소요 시간 : {model.total_train_time}\")\n",
    "print(model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e03b2-fed8-4bae-9f22-b96f450acbd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 모델 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f2aaf4-db95-431e-a42e-b12a5609627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 16:35:18,404 : INFO : loading FastText object from text_correciton_relation_model\n",
      "2022-09-01 16:35:18,407 : INFO : loading wv recursively from text_correciton_relation_model.wv.* with mmap=None\n",
      "2022-09-01 16:35:18,408 : INFO : loading vectors_ngrams from text_correciton_relation_model.wv.vectors_ngrams.npy with mmap=None\n",
      "2022-09-01 16:35:19,513 : INFO : setting ignored attribute buckets_word to None\n",
      "2022-09-01 16:35:19,514 : INFO : setting ignored attribute vectors to None\n",
      "2022-09-01 16:35:19,517 : INFO : setting ignored attribute cum_table to None\n",
      "2022-09-01 16:35:19,519 : INFO : FastText lifecycle event {'fname': 'text_correciton_relation_model', 'datetime': '2022-09-01T16:35:19.519228', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "correction_model = FastText.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1966d-d313-42c7-a58b-3938ba452dd3",
   "metadata": {},
   "source": [
    "## 공통 Function\n",
    "- word_sim : 단어 유사도 측정  \n",
    "- jamo_word : 자모 단위로 단어를 분해하여 유사도 측정하여 성능 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286e76dd-3a2a-4956-8b06-063b51d12cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 단어 리스트 사이의 유사도 측정\n",
    "def word_sim(word_A, word_b, model=correction_model):\n",
    "    return model.wv.n_similarity(word_A, word_b)\n",
    "\n",
    "\n",
    "# 단어를 자모 단위로 분해\n",
    "def jamo_word(sent):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "\n",
    "    def transform(char):\n",
    "        try:\n",
    "            if char == ' ':\n",
    "                return char\n",
    "            jamo = decompose(char)\n",
    "            if len(jamo) == 1:\n",
    "                return jamo\n",
    "            jamo_ = ''.join(c if c != ' ' else 'e' for c in jamo)\n",
    "            return jamo_\n",
    "        \n",
    "        except Exception as e: # 마침표, 물음표 반환\n",
    "            return char\n",
    "\n",
    "    sent_ = ''.join(transform(char) for char in sent)\n",
    "    sent_ = doublespace_pattern.sub(' ', sent_)\n",
    "    return sent_\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae75a6-90cb-452f-ba45-982b3bafce6e",
   "metadata": {},
   "source": [
    "## 단어 사전과 비교하여 유사성 검사\n",
    "- **jamo 분리해서 유사성 검증하면 정확도 향상**  \n",
    "- 글자수 같은 것끼리만 판단하도록 적용  \n",
    "    &rarr; 실제 오류 케이스 확인하여 1~2글자는 글자수 같을 때만 체크하고 나머지는 전체 비교할지 등 보완"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76704e1b-ff6c-429b-9f81-f0892a8fd158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 단어: 몬인\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>본인</td>\n",
       "      <td>0.999105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>증손</td>\n",
       "      <td>0.997497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>장인</td>\n",
       "      <td>0.997158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relation     score\n",
       "0       본인  0.999105\n",
       "1       증손  0.997497\n",
       "2       장인  0.997158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '몬인'\n",
    "dictionary = ['본인', '배우자', '부', '모', '자녀', '며느리', '사위', '시부', '시모', '장인', '장모', '백부', '백모', '숙부', '숙모', '양부', '양모', '형', '형수', '제', '제수', '시숙', '올케', '종형', '종형수', '종제', '종제수', '누이', '형부', '매', '매부', '시누이', '종매', '종매부', '제부', '조카', '동서', '손', '손부', '손서', '증손', '증손부', '고손', '고손부', '고종', '오빠', '배우자의자', '조부', '조모', '시조부', '시조모', '증조부', '증조모', '고조부', '고조모', '장조부', '장조모', '고모', '고모부', '대고모', '당숙', '당숙모', '서모', '서조모', '이질', '당질', '질부', '외숙', '외숙모', '외종', '이모', '이모부', '이종', '외손', '외손부', '외조부', '외조모', '처형', '처제', '처남', '처남댁', '처조카', '처질부', '처숙부', '처숙모', '처사촌', '처고모', '처고모부', '처고종', '처이모', '처이모부', '처이종', '처가친척', '시가친척', '외가친척', '친척', '동거인', '외증손', '외증손부', '언니', '시외조부', '시외조모', '누나', '계부', '계모']\n",
    "results = []\n",
    "\n",
    "for i in range(len(dictionary)):\n",
    "    \n",
    "    if len(word) == len(dictionary[i]):\n",
    "        result_info = {}\n",
    "        result_info['relation'] = dictionary[i]\n",
    "        result_info['score'] = word_sim(jamo_word(word), jamo_word(dictionary[i]), correction_model) \n",
    "        #result_info['score'] = word_sim(dictionary[i], word, saved_model) \n",
    "        results.append(result_info)\n",
    "        \n",
    "results = sorted(results, key=lambda d:d['score'], reverse=True)\n",
    "df = pd.DataFrame(results)\n",
    "print('입력 단어:', word)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b48b5-f80e-4e83-b45b-acecf57ceb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
